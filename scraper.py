import os
import feedparser
import re
import urllib.parse
from supabase import create_client

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø±Ø¨Ø·
URL = os.getenv("SUPABASE_URL")
KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(URL, KEY)

def get_unique_tech_image(title):
    """Ø¬Ù„Ø¨ ØµÙˆØ±Ø© Ù…Ù…ÙŠØ²Ø© ÙˆØ¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†"""
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØªØ±Ø¬Ù…Ø© Ø¨Ø¹Ø¶ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¨Ø­Ø«
    clean_title = re.sub(r'[^\w\s]', '', title)
    # ÙƒÙ„Ù…Ø§Øª Ø¯Ù„Ø§Ù„ÙŠØ© ØªØ¬Ø¹Ù„ Ø§Ù„ØµÙˆØ± Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø±Ù…Ø¬ÙŠØ© ÙˆØªÙ‚Ù†ÙŠØ©
    tech_keywords = "programming,coding,technology,software"
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¥Ù„Ù‰ ØµÙŠØºØ© URL
    encoded_query = urllib.parse.quote(f"{tech_keywords},{clean_title}")
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… sig Ù„Ø¬Ø¹Ù„ Unsplash ÙŠÙˆÙ„Ø¯ ØµÙˆØ±Ø© ÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ Ø®Ø¨Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    return f"https://images.unsplash.com/featured/?{encoded_query}&sig={len(title) + hash(title)}"

def start_scraping():
    sources = [
        {"url": "https://aitnews.com/category/Ø¨Ø±Ù…Ø¬ÙŠØ§Øª-ÙˆØ¹Ù„ÙˆÙ…-Ø­Ø§Ø³Ø¨/feed/", "cat": "Ø¨Ø±Ù…Ø¬ÙŠØ§Øª"},
        {"url": "https://www.tech-wd.com/wd/category/programming/feed/", "cat": "Ø¨Ø±Ù…Ø¬Ø©"},
        {"url": "https://www.unlimit-tech.com/category/programming/feed/", "cat": "ØªØ·ÙˆÙŠØ±"}
    ]
    
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø¨ØµÙˆØ± Ù…Ù…ÙŠØ²Ø©...")
    
    for source in sources:
        feed = feedparser.parse(source['url'])
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹
        author_name = feed.feed.title.split('-')[0].strip() if 'title' in feed.feed else "Ù…ØµØ¯Ø± ØªÙ‚Ù†ÙŠ"
        
        for entry in feed.entries[:15]:
            # 1. Ø¬Ù„Ø¨ ØµÙˆØ±Ø© Ù…Ù…ÙŠØ²Ø© ÙˆÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ Ø®Ø¨Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ù†ÙˆØ§Ù†Ù‡
            img_url = get_unique_tech_image(entry.title)

            # 2. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content_raw = entry.summary if 'summary' in entry else entry.title
            content_clean = re.sub(r'<[^>]+>', '', content_raw).strip()

            news_data = {
                "title": entry.title,
                "image_url": img_url,
                "content": content_clean,
                "author": author_name,
                "category": source['cat']
            }
            
            try:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø®Ø¨Ø± Ø£Ùˆ Ø¥Ø¶Ø§ÙØªÙ‡
                supabase.table("academy_news").upsert(news_data, on_conflict='title').execute()
            except Exception as e:
                continue
                
    print("âœ… ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ«! Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¢Ù† Ø³ØªÙƒÙˆÙ† Ù…Ù…ÙŠØ²Ø© ÙˆÙ…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ Ø®Ø¨Ø±.")

if __name__ == "__main__":
    start_scraping()
