import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client

# Ø³Ø­Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª GitHub Secrets (Ù„Ù„Ø£Ù…Ø§Ù†)
URL = os.getenv("SUPABASE_URL")
KEY = os.getenv("SUPABASE_KEY")

if not URL or not KEY:
    print("âŒ Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª GitHub")
    exit(1)

supabase = create_client(URL, KEY)

def scrape_and_upload():
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø±...")
    target_url = "https://www.tech-wd.com/wd/category/news/"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(target_url, headers=headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = soup.select('article')[:5] 
        
        news_to_insert = []
        for art in articles:
            title_tag = art.select_one('h2')
            if title_tag:
                news_to_insert.append({
                    "title": title_tag.text.strip(),
                    "image_url": "https://via.placeholder.com/150", # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø³ÙŠÙ† Ø³Ø­Ø¨ Ø§Ù„ØµÙˆØ± Ù„Ø§Ø­Ù‚Ø§Ù‹
                    "content": "ØªÙ… Ø¬Ù„Ø¨ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨Ø± Ø¢Ù„ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©."
                })

        if news_to_insert:
            supabase.table("academy_news").insert(news_to_insert).execute()
            print(f"âœ… ØªÙ… Ø±ÙØ¹ {len(news_to_insert)} Ø®Ø¨Ø± Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")

if __name__ == "__main__":
    scrape_and_upload()
