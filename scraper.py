import os
import feedparser
import re
import random
from supabase import create_client

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø±Ø¨Ø·
URL = os.getenv("SUPABASE_URL")
KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(URL, KEY)

def get_joker_image():
    """Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø§Ù„Ø«Ø§Ø¨ØªØ© - Ø§Ø®ØªØ±Ù†Ø§ Ù„Ùƒ Ø£ÙØ¶Ù„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©"""
    pro_images = [
        "https://images.unsplash.com/photo-1461749280684-dccba630e2f6?w=800&q=80",
        "https://images.unsplash.com/photo-1498050108023-c5249f4df085?w=800&q=80",
        "https://images.unsplash.com/photo-1504639725590-34d0984388bd?w=800&q=80",
        "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?w=800&q=80",
        "https://images.unsplash.com/photo-1587620962725-abab7fe55159?w=800&q=80",
        "https://images.unsplash.com/photo-1542831371-29b0f74f9713?w=800&q=80",
        "https://images.unsplash.com/photo-1550751827-4bd374c3f58b?w=800&q=80",
        "https://images.unsplash.com/photo-1517694712202-14dd9538aa97?w=800&q=80",
        "https://images.unsplash.com/photo-1555066931-4365d14bab8c?w=800&q=80",
        "https://images.unsplash.com/photo-1516116216624-53e697fedbea?w=800&q=80"
    ]
    # Ø§Ø®ØªÙŠØ§Ø± ØµÙˆØ±Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù…Ù…Ù„
    return random.choice(pro_images)

def start_scraping():
    # Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙ‚Ù†ÙŠØ©
    sources = [
        {"url": "https://aitnews.com/category/Ø¨Ø±Ù…Ø¬ÙŠØ§Øª-ÙˆØ¹Ù„ÙˆÙ…-Ø­Ø§Ø³Ø¨/feed/", "cat": "Ø¨Ø±Ù…Ø¬ÙŠØ§Øª"},
        {"url": "https://www.tech-wd.com/wd/category/programming/feed/", "cat": "Ø¨Ø±Ù…Ø¬Ø©"},
        {"url": "https://arabhardware.net/news/feed", "cat": "Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØªÙ‚Ù†ÙŠØ©"}
    ]
    
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬ÙˆÙƒØ± Ù„Ù„ØµÙˆØ±...")
    
    for source in sources:
        feed = feedparser.parse(source['url'])
        author_name = feed.feed.title.split('-')[0].strip() if 'title' in feed.feed else "Ù…ØµØ¯Ø± ØªÙ‚Ù†ÙŠ"
        
        for entry in feed.entries[:12]:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬ÙˆÙƒØ± Ù„Ù„ØµÙˆØ± Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©
            img_url = get_joker_image()

            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† ÙˆØ³ÙˆÙ… HTML
            content_raw = entry.summary if 'summary' in entry else entry.title
            content_clean = re.sub(r'<[^>]+>', '', content_raw).strip()
            if len(content_clean) > 250:
                content_clean = content_clean[:247] + "..."

            news_data = {
                "title": entry.title,
                "image_url": img_url,
                "content": content_clean,
                "author": author_name,
                "category": source['cat']
            }
            
            try:
                # Ø§Ù„Ø±ÙØ¹ Ù„Ù€ Supabase
                supabase.table("academy_news").upsert(news_data, on_conflict='title').execute()
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø®Ø¨Ø±: {entry.title[:20]}..")
                continue
                
    print("âœ… ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ«! Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¢Ù† Ù…Ø³ØªÙ‚Ø±Ø© 100% ÙˆØªØ¸Ù‡Ø± Ù„Ù„Ø¬Ù…ÙŠØ¹.")

if __name__ == "__main__":
    start_scraping()
