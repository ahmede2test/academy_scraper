import os
import feedparser
import re
from supabase import create_client

URL = os.getenv("SUPABASE_URL")
KEY = os.getenv("SUPABASE_KEY")
supabase = create_client(URL, KEY)

def extract_best_image(entry):
    """Ø¯Ø§Ù„Ø© Ø°ÙƒÙŠØ© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø· ØµÙˆØ±Ø© Ù…ØªØ§Ø­ Ù„Ù„Ø®Ø¨Ø±"""
    # 1. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙˆØ³Ù… media_content
    if 'media_content' in entry and len(entry.media_content) > 0:
        return entry.media_content[0]['url']
    
    # 2. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· (Enclosures)
    if 'links' in entry:
        for link in entry.links:
            if 'image' in link.get('type', ''):
                return link.href

    # 3. Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„ÙˆØµÙ Ø£Ùˆ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Regex (Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
    search_text = ""
    if 'content' in entry:
        search_text = entry.content[0].value
    elif 'summary' in entry:
        search_text = entry.summary
    
    if search_text:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ Ø±Ø§Ø¨Ø· ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ø§Ù…ØªØ¯Ø§Ø¯ ØµÙˆØ±Ø©
        match = re.search(r'src="([^"]+\.(?:jpg|png|jpeg|webp|gif)[^"]*)"', search_text)
        if match:
            return match.group(1)

    # 4. ØµÙˆØ±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© "Ù…ØªØºÙŠØ±Ø©" ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø®Ø¨Ø± Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙƒØ±Ø§Ø±
    # Ù†Ø³ØªØ®Ø¯Ù… sig Ù„Ø¬Ø¹Ù„ Unsplash ÙŠØ¹Ø·ÙŠ ØµÙˆØ±Ø© Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ Ø®Ø¨Ø±
    return f"https://images.unsplash.com/photo-1517694712202-14dd9538aa97?q=80&w=1000&sig={hash(entry.title)}"

def start_scraping():
    sources = [
        {"url": "https://aitnews.com/category/Ø¨Ø±Ù…Ø¬ÙŠØ§Øª-ÙˆØ¹Ù„ÙˆÙ…-Ø­Ø§Ø³Ø¨/feed/", "cat": "Ø¨Ø±Ù…Ø¬ÙŠØ§Øª"},
        {"url": "https://www.tech-wd.com/wd/category/programming/feed/", "cat": "Ø¨Ø±Ù…Ø¬Ø©"},
        {"url": "https://www.unlimit-tech.com/category/programming/feed/", "cat": "ØªØ·ÙˆÙŠØ±"}
    ]
    
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø¨Ø¯Ø¡ Ø³Ø­Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨ØµÙˆØ±Ù‡Ø§ Ø§Ù„Ø£ØµÙ„ÙŠØ©...")
    
    for source in sources:
        feed = feedparser.parse(source['url'])
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø´ÙƒÙ„ Ø£Ù†Ø¸Ù
        author_name = feed.feed.title.split('-')[0].strip() if 'title' in feed.feed else "Ù…ØµØ¯Ø± ØªÙ‚Ù†ÙŠ"
        
        for entry in feed.entries[:15]:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©
            img_url = extract_best_image(entry)

            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† ÙˆØ³ÙˆÙ… HTML ØªÙ…Ø§Ù…Ø§Ù‹
            content_raw = entry.summary if 'summary' in entry else entry.title
            content_clean = re.sub(r'<[^>]+>', '', content_raw).strip()

            news_data = {
                "title": entry.title,
                "image_url": img_url,
                "content": content_clean,
                "author": author_name,
                "category": source['cat']
            }
            
            try:
                # ØªØ­Ø¯ÙŠØ« Ù„Ùˆ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…ÙˆØ¬ÙˆØ¯ Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© Ù„Ùˆ Ø¬Ø¯ÙŠØ¯
                supabase.table("academy_news").upsert(news_data, on_conflict='title').execute()
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø±ÙØ¹ Ø®Ø¨Ø±: {entry.title[:20]}... : {e}")
                continue
                
    print("âœ… ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ù†Ø¬Ø§Ø­! Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¢Ù† ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…ØªÙ†ÙˆØ¹Ø©.")

if __name__ == "__main__":
    start_scraping()
